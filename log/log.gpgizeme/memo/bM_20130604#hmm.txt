



== R Package (HMM) ==

 <pre>
_ver=20130603_014942
_ver=20130604_002222
</pre>


* http://cran.r-project.org/web/packages/HMM/HMM.pdf
* February 15, 2013
* Maintainer: Lin Himmelmann <hmm@linhi.com>
* Depends: R (>= 2.0.0)
* Description: easy to use library to setup, apply and make inference with discrete time and discrete space Hidden Markov Models




=== backward ===
: Computes the backward probabilities.

* The backward probability for state X and observation at time k is defined as the probability of observing the sequence of observations e_k+1, ..., e_n under the condition that the state at time k is X.
: b[X,k] := Prob(E_k+1 = e_k+1, ..., E_n = e_n | X_k = X)
: , where E_k+1 ... E_n = e_k+1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k.

* Usage
 <pre>
# Initialise HMM
hmm = initHMM(c("A","B"), c("L","R"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))
print(hmm)

# Sequence of observations
observations = c("L","L","R","R")

# Calculate backward probablities
logBackwardProbabilities = backward(hmm,observations)
print(exp(logBackwardProbabilities))
</pre>




=== baumWelch ===
: Inferring the parameters of a HMM via the Baum-Welch algorithm

* For an initial HMM and a given sequence of observations, the Baum-Welch algorithm infers optimal parameters to the HMM. Since the Baum-Welch algorithm is a variant of the Expectation-Maximisation algorithm, the algorithm converges to a local solution which might not be the global optimum.

* Arguments
:- hmm: A Hidden Markov Model
:- observation: A sequence of observations
:- maxIterations: The maximum number of iterations in the Baum-Welch algorithm
:- delta: Additional termination condition, if the transition and emission matrices converge, before reaching the maximum number of iterations (maxIterations). The difference of transition and emission parameters in consecutive iterations must be smaller than delta to terminate the algorithm
:- pseudoCount: Adding this amount of pseudo counts in the estimation-step of the Baum-Welch algorithm

* Return Values
:- hmm: The inferred HMM, of which representation is equivalent to the representation in initHMM()
:- difference: Vector of differences calculated from consequtive transition and emission matrices in each iteration of the Baum-Welch procedure. The difference is the sum of the distance between consecutive transition and emission matrices in the L2-Norm

* Usage
 <pre>
# Initial HMM
hmm = initHMM(c("A","B"),c("L","R"), transProbs=matrix(c(.9,.1,.1,.9),2), emissionProbs=matrix(c(.5,.51,.5,.49),2))
print(hmm)

# Sequence of observation
a = sample(c(rep("L",100),rep("R",300)))
b = sample(c(rep("L",300),rep("R",100)))
observation = c(a,b)

# Baum-Welch
bw = baumWelch(hmm,observation,10) # bw <- baumWelch(hmm, observation, maxIterations=100, delta=1E-9, pseudoCount=0)
print(bw$hmm) # the inferred HMM // representation is equivalent to the representation in initHMM()
print(bw$difference)
</pre>




=== dishonestCasino ===
: Example application for HMM

* The dishonest casino gives an example for the application of Hidden Markov Models. This example is taken from Durbin et. al. 1999: A dishonest casino uses two dice, one of them is fair the other is loaded. The probabilities of the fair die are (1/6,...,1/6) for throwing ("1",...,"6"). The probabilities of the loaded die are (1/10,...,1/10,1/2) for throwing ("1",..."5","6"). The observer doesnâ€™t know which die is actually taken (the state is hidden), but the sequence of throws (observations) can be used to infer which die (state) was used.

* Usage
 <pre>
dishonestCasino()
</pre>

* Source Code
 <pre>
dishonestCasino <- function () {
    nSim = 2000
    States = c("Fair", "Unfair")
    Symbols = 1:6
    transProbs = matrix(c(0.99, 0.01, 0.02, 0.98), c(length(States), 
        length(States)), byrow = TRUE)
    emissionProbs = matrix(c(rep(1/6, 6), c(rep(0.1, 5), 0.5)), 
        c(length(States), length(Symbols)), byrow = TRUE)
    hmm = initHMM(States, Symbols, transProbs = transProbs, emissionProbs = emissionProbs)
    sim = simHMM(hmm, nSim)
    vit = viterbi(hmm, sim$observation)
    f = forward(hmm, sim$observation)
    b = backward(hmm, sim$observation)
    i <- f[1, nSim]
    j <- f[2, nSim]
    probObservations = (i + log(1 + exp(j - i)))
    posterior = exp((f + b) - probObservations)
    x = list(hmm = hmm, sim = sim, vit = vit, posterior = posterior)
    readline("Plot simulated throws:\n")
    mn = "Fair and unfair die"
    xlb = "Throw nr."
    ylb = ""
    plot(x$sim$observation, ylim = c(-7.5, 6), pch = 3, main = mn, 
        xlab = xlb, ylab = ylb, bty = "n", yaxt = "n")
    axis(2, at = 1:6)
    readline("Simulated, which die was used:\n")
    text(0, -1.2, adj = 0, cex = 0.8, col = "black", "True: green = fair die")
    for (i in 1:nSim) {
        if (x$sim$states[i] == "Fair") 
            rect(i, -1, i + 1, 0, col = "green", border = NA)
        else rect(i, -1, i + 1, 0, col = "red", border = NA)
    }
    readline("Most probable path (viterbi):\n")
    text(0, -3.2, adj = 0, cex = 0.8, col = "black", "Most probable path")
    for (i in 1:nSim) {
        if (x$vit[i] == "Fair") 
            rect(i, -3, i + 1, -2, col = "green", border = NA)
        else rect(i, -3, i + 1, -2, col = "red", border = NA)
    }
    readline("Differences:\n")
    text(0, -5.2, adj = 0, cex = 0.8, col = "black", "Difference")
    differing = !(x$sim$states == x$vit)
    for (i in 1:nSim) {
        if (differing[i]) 
            rect(i, -5, i + 1, -4, col = rgb(0.3, 0.3, 0.3), 
                border = NA)
        else rect(i, -5, i + 1, -4, col = rgb(0.9, 0.9, 0.9), 
            border = NA)
    }
    readline("Posterior-probability:\n")
    points(x$posterior[2, ] - 3, type = "l")
    readline("Difference with classification by posterior-probability:\n")
    text(0, -7.2, adj = 0, cex = 0.8, col = "black", "Difference by posterior-probability")
    differing = !(x$sim$states == x$vit)
    for (i in 1:nSim) {
        if (posterior[1, i] > 0.5) {
            if (x$sim$states[i] == "Fair") 
                rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), 
                  border = NA)
            else rect(i, -7, i + 1, -6, col = rgb(0.3, 0.3, 0.3), 
                border = NA)
        }
        else {
            if (x$sim$states[i] == "Unfair") 
                rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), 
                  border = NA)
            else rect(i, -7, i + 1, -6, col = rgb(0.3, 0.3, 0.3), 
                border = NA)
        }
    }
    readline("Difference with classification by posterior-probability > .95:\n")
    text(0, -7.2, adj = 0, cex = 0.8, col = "black", "Difference by posterior-probability > .95")
    differing = !(x$sim$states == x$vit)
    for (i in 1:nSim) {
        if (posterior[2, i] > 0.95 || posterior[2, i] < 0.05) {
            if (differing[i]) 
                rect(i, -7, i + 1, -6, col = rgb(0.3, 0.3, 0.3), 
                  border = NA)
            else rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), 
                border = NA)
        }
        else {
            rect(i, -7, i + 1, -6, col = rgb(0.9, 0.9, 0.9), 
                border = NA)
        }
    }
    invisible(x)
}
<environment: namespace:HMM>
</pre>




=== forward ===
: Computes the forward probabilities.

* The forward probability for state X up to observation at time k is defined as the probability of observing the sequence of observations e_1, e_2, ..., e_k and that the state at time k is X.
: f[X,k] : = Prob(E_1 = e_1, ..., E_k = e_k, X_k = X)
: , where E_1 ... E_n = e_1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k.

* Usage
 <pre>
# Initialise HMM
hmm = initHMM(c("A","B"), c("L","R"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))
print(hmm)

# Sequence of observations
observations = c("L","L","R","R")

# Calculate forward probablities
logForwardProbabilities = forward(hmm,observations)
print(exp(logForwardProbabilities))
</pre>




=== HMM ===
: HMM - Hidden Markov Models

* Modelling, analysis and inference with discrete time and discrete space Hidden Markov Models.




=== initHMM ===
: Initialization of HMM's

* This function initialises a general discrete time and discrete space Hidden Markov Model (HMM).  A HMM consists of an alphabet of states and emission symbols. A HMM assumes that the states are hidden from the observer, while only the emissions of the states are observable. The HMM is de- signed to make inference on the states through the observation of emissions. The stochastics of the HMM is fully described by the initial starting probabilities of the states, the transition probabilities between states and the emission probabilities of the states.

* The function initHMM returns a HMM that consists of a list of 5 elements:
:- States: Vector with the names of the states.
:- Symbols: Vector with the names of the symbols.
:- startProbs: Annotated vector with the starting probabilities of the states.
:- transProbs: Annotated matrix containing the transition probabilities between the states.
:- emissionProbs: Annotated matrix containing the emission probabilities of the states.

* Usage
 <pre>
# function interface
initHMM(States, Symbols, startProbs=NULL, transProbs=NULL, emissionProbs=NULL)

# Initialise HMM (example 1)
hmm_1 <- initHMM(c("X","Y"), c("a","b","c"))

# Initialise HMM (example 2)
hmm_2 <- initHMM(c("X","Y"), c("a","b"), c(.3,.7), matrix(c(.9,.1,.1,.9),2), matrix(c(.3,.7,.7,.3),2))
</pre>




=== posterior ===
: Computes the posterior probabilities for the states

* This function computes the posterior probabilities of being in state X at time k for a given sequence of observations and a given HMM.

* The posterior probability of being in a state X at time k can be computed from the forward and backward probabilities:
: Ws(X_k = X | E_1 = e_1, ..., E_n = e_n) = f[X,k] * b[X,k] / Prob(E_1 = e_1, ..., E_n = e_n)
: , where E_1 ... E_n = e_1 ... e_n is the sequence of observed emissions and X_k is a random variable that represents the state at time k

* Usage
 <pre>
# Initialise HMM
hmm = initHMM(c("A","B"), c("L","R"), transProbs=matrix(c(.8,.2,.2,.8),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))
print(hmm)

# Sequence of observations
observations = c("L","L","R","R")

# Calculate posterior probablities of the states
posterior = posterior(hmm,observations)
print(posterior)
</pre>




=== simHMM ===
: Simulates states and observations for a HMM

* Simulates a path of states and observations for a given HMM

* Usage
 <pre>
# Initialise HMM
hmm = initHMM(c("X","Y"),c("a","b","c")) # does this provide enough initial conditions for simulation?

# Simulate from the HMM
simHMM(hmm, 100)
</pre>




=== viterbi ===
: Computes the most probable path of states

* The Viterbi-algorithm computes the most probable path of states for a sequence of observations for a given HMM.

* Usage
 <pre>
# Initialise HMM
hmm = initHMM(c("A","B"), c("L","R"), transProbs=matrix(c(.6,.4,.4,.6),2), emissionProbs=matrix(c(.6,.4,.4,.6),2))
print(hmm)

# Sequence of observations
observations = c("L","L","R","R")

# Calculate Viterbi path
viterbiPath = viterbi(hmm,observations)
print(viterbiPath)
</pre>




=== viterbiTraining ===
: Inferring the parameters of a HMM via Viterbi-training

* For an initial Hidden Markov Model (HMM) and a given sequence of observations, the Viterbitraining algorithm infers optimal parameters to the HMM. Viterbi-training usually converges much faster than the Baum-Welch algorithm, but the underlying algorithm is theoretically less justified.  Be careful: The algorithm converges to a local solution which might not be the optimum.

* Arguments
:- hmm: A Hidden Markov Model
:- observation: A sequence of observations
:- maxIterations: The maximum number of iterations in the Viterbi-training algorithm
:- delta: Additional termination condition, if the transition and emission matrices converge, before reaching the maximum number of iterations (maxIterations). The difference of transition and emission parameters in consecutive iterations must be smaller than delta to terminate the algorithm
:- pseudoCount: Adding this amount of pseudo counts in the estimation-step of the Viterbi-training algorithm

* Return Values
:- hmm: The inferred HMM, of which representation is equivalent to the representation in initHMM()
:- difference: Vector of differences calculated from consequtive transition and emission matrices in each iteration of the Viterbi-training. The difference is the sum of the distance between consecutive transition and emission matrices in the L2-Norm

* Usage
 <pre>
# Initial HMM
hmm = initHMM(c("A","B"),c("L","R"), transProbs=matrix(c(.9,.1,.1,.9),2), emissionProbs=matrix(c(.5,.51,.5,.49),2))
print(hmm)

# Sequence of observation
a = sample(c(rep("L",100),rep("R",300)))
b = sample(c(rep("L",300),rep("R",100)))
observation = c(a,b)

# Viterbi-training
vt = viterbiTraining(hmm,observation,10)
print(vt$hmm)
</pre>




