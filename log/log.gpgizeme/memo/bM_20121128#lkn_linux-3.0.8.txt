##--------------------------
##LKN: Linux Kernel Note
##linux-3.0.8
##--------------------------
##tstamp: 20121128_191520
##tstamp: 20121204_011524
##tstamp: 20121206_012417
##_ver="20121207_210155";
##_ver="20121211_111831";
##--------------------------




//{ FREENOTE #--------------------------------------------------------------
//
//
//





/* hand a buffer to its device driver for I/O */
static inline void __generic_make_request(struct bio *bio)
{
/**
 * generic_make_request - hand a buffer to its device driver for I/O
 * @bio:  The bio describing the location in memory and on the device.
 *
 * generic_make_request() is used to make I/O requests of block
 * devices. It is passed a &struct bio, which describes the I/O that needs
 * to be done.
 *
 * generic_make_request() does not return any status.  The
 * success/failure status of the request, along with notification of
 * completion, is delivered asynchronously through the bio->bi_end_io
 * function described (one day) else where.
 *
 * The caller of generic_make_request must make sure that bi_io_vec
 * are set to describe the memory buffer, and that bi_dev and bi_sector are
 * set to describe the device address, and the
 * bi_end_io and optionally bi_private are set to describe how
 * completion notification should be signaled.
 *
 * generic_make_request and the drivers it calls may use bi_next if this
 * bio happens to be merged with someone else, and may change bi_dev and
 * bi_sector for remaps as it sees fit.  So the values of these fields
 * should NOT be depended on after the call to generic_make_request.
 */




}



/*
 * BIO list management for use by remapping drivers (e.g. DM or MD) and loop.
 *
 * A bio_list anchors a singly-linked list of bios chained through the bi_next
 * member of the bio.  The bio_list also caches the last list member to allow
 * fast access to the tail.
 */
include/linux/bio.h: struct bio_list {
	struct bio *head;
	struct bio *tail;
};




include/linux/bio.h: static inline void bio_list_add(struct bio_list *bl, struct bio *bio)
{
	bio->bi_next = NULL;
	if (bl->tail)
		bl->tail->bi_next = bio;
	else
		bl->head = bio;
	bl->tail = bio;
}




include/linux/bio.h: static inline void bio_list_init(struct bio_list *bl)
{
	bl->head = bl->tail = NULL;
}




include/linux/bio.h: static inline struct bio *bio_list_pop(struct bio_list *bl)
{
/*
 * returns the head of bio_list 'bl'	// bl->head
 * returns null ((struct bio *)0) if the head is not exist
 *
 * if the head (bl->head) was not null,
 *	bl->head will be replaced by 'the next item' // bl->head = bl->head->bi_next;
 *	if 'the next item' is NULL, null value will be assigned to bl->tail.
 *	and request queue link of the bio item pop'ed will be assigned to NULL.
 */
	struct bio *bio = bl->head;

	if (bio) {
		bl->head = bl->head->bi_next;
		if (!bl->head)
			bl->tail = NULL;
		bio->bi_next = NULL;
	}
	return bio;
}


block/blk-core.c: void generic_make_request(struct bio *bio)
{
/* pop and then process bio iteratively */
	struct bio_list bio_list_on_stack;

	if (current->bio_list) {
		/* make_request is active */
		bio_list_add(current->bio_list, bio);
		return;
	}

	BUG_ON(bio->bi_next);
	bio_list_init(&bio_list_on_stack);
	current->bio_list = &bio_list_on_stack;
	do {
		__generic_make_request(bio);
		bio = bio_list_pop(current->bio_list);
	} while (bio);
	current->bio_list = NULL; /* deactivate */
}







anon_vma {
};

PAGE_MAPPING_ANON

struct page {};
struct address_space {};
struct address_space_operations {};


fs/mpage.c: do_mpage_readpage()
	<- fs/mpage.c: mpage_readpage()
	<- fs/mpage.c: mpage_readpages()

block/blk-core.c: submit_bio()
	<- fs/mpage.c: mpage_bio_submit()

fs/mpage.c: mpage_bio_submit()
	<- fs/mpage.c: mpage_readpage()

fs/mpage.c: mpage_readpage()
	<- fs/ext4/inode.c: ext4_readpage()




//{ ftrace note: original unmodified
ftrace{	logfile: ftlog.20121218_195819.pid_3718.log
	foo@bar$ dd if=/x/bigfile/f3 of=/dev/null skip=0 bs=512 count=1;
	#------------------------------------------------------------------------#


	sys_read <-system_call_fastpath;	// 6590.428417 // line:20876
	...;
	vfs_read <-sys_read;	// 6590.428417 // line:20878
		rw_verify_area <-vfs_read;
			security_file_permission <-rw_verify_area;
				apparmor_file_permission <-security_file_permission;
					common_file_perm <-apparmor_file_permission;
				__fsnotify_parent <-security_file_permission;
				fsnotify <-security_file_permission;
		do_sync_read <-vfs_read;	// 6590.428419 // line:20885
			generic_file_aio_read <-do_sync_read; // 6590.428419 // line:20886
				generic_segment_checks <-generic_file_aio_read;
				blk_start_plug <-generic_file_aio_read;
				printk <-generic_file_aio_read;
					vprintk <-printk;
						_raw_spin_lock <-vprintk;
						// __LOOP__ { xN
						emit_log_char <-vprintk;
						// }
						...;
				find_get_page <-generic_file_aio_read;	// 6590.428472 // line:21213
				page_cache_sync_readahead <-generic_file_aio_read;	// line:21214
					ondemand_readahead <-page_cache_sync_readahead;	// line:21215
						max_sane_readahead <-ondemand_readahead;
						get_init_ra_size <-ondemand_readahead;
						ra_submit <-ondemand_readahead;
							__do_page_cache_readahead <-ra_submit;	// line:21219
								// __LOOP__ { x4
								__page_cache_alloc <-__do_page_cache_readahead;	// line:21220
									alloc_pages_current <-__page_cache_alloc;
										policy_nodemask <-alloc_pages_current;
										policy_zonelist <-alloc_pages_current;
										__alloc_pages_nodemask <-alloc_pages_current;
											_cond_resched <-__alloc_pages_nodemask;
											next_zones_zonelist <-__alloc_pages_nodemask;
											get_page_from_freelist <-__alloc_pages_nodemask;
												next_zones_zonelist <-get_page_from_freelist;
												zone_watermark_ok <-get_page_from_freelist;
												zone_statistics <-get_page_from_freelist;
													__inc_zone_state <-zone_statistics;
													__inc_zone_state <-zone_statistics;	// line:21232
								// } // lines:21220~21271
								blk_start_plug <-__do_page_cache_readahead;	// line:21272
								ext4_readpages <-__do_page_cache_readahead;	// line:21273
									mpage_readpages <-ext4_readpages;	// line:21274
										blk_start_plug <-mpage_readpages;
										add_to_page_cache_lru <-mpage_readpages;	// line:21276
											add_to_page_cache_locked <-add_to_page_cache_lru;
												mem_cgroup_cache_charge <-add_to_page_cache_locked;
											...;
										do_mpage_readpage <-mpage_readpages;	// 6590.428487 // line:21292
											ext4_get_block <-do_mpage_readpage; // 6590.428487 // line:21293
												_ext4_get_block <-ext4_get_block;
													ext4_map_blocks <-_ext4_get_block;
														down_read <-ext4_map_blocks;
														ext4_ext_map_blocks <-ext4_map_blocks;
															ext4_ext_check_cache <-ext4_ext_map_blocks;
															ext4_ext_find_extent <-ext4_ext_map_blocks;
																__kmalloc <-ext4_ext_find_extent;
																	get_slab <-__kmalloc;
																__getblk <-ext4_ext_find_extent;
																	// __LOOP__ {
																	__find_get_block <-__getblk;
																		find_get_page <-__find_get_block_slow.isra.17 <-__find_get_block;
																	// }
																	__find_or_create_page <-__getblk;
																		find_lock_page <-__find_or_create_page;
																			find_get_page <-find_lock_page;
																		__page_cache_alloc <-__find_or_create_page;
																			alloc_pages_current <-__page_cache_alloc;
																				policy_nodemask <-alloc_pages_current;
																				policy_zonelist <-alloc_pages_current;
																				__alloc_pages_nodemask <-alloc_pages_current;
																					next_zones_zonelist <-__alloc_pages_nodemask;
																					get_page_from_freelist <-__alloc_pages_nodemask;
																						next_zones_zonelist <-get_page_from_freelist;
																						zone_watermark_ok <-get_page_from_freelist;
																						zone_statistics <-get_page_from_freelist;
																							// __LOOP__ { // #?? is this loop? or just two-time run?
																							__inc_zone_state <-zone_statistics;
																							// }
																		add_to_page_cache_lru <-__find_or_create_page;
																			add_to_page_cache_locked <-add_to_page_cache_lru;
																				mem_cgroup_cache_charge <-add_to_page_cache_locked;
																					__mem_cgroup_try_charge <-mem_cgroup_cache_charge;
																					__mem_cgroup_commit_charge_lrucare <-mem_cgroup_cache_charge;
																						// __LOOP__ { // #?? is this loop? or just two-time run?
																						lookup_page_cgroup <-__mem_cgroup_commit_charge_lrucare;
																						// }
																						__mem_cgroup_commit_charge <-__mem_cgroup_commit_charge_lrucare;
																							mem_cgroup_charge_statistics.isra.27 <-__mem_cgroup_commit_charge;
																							memcg_check_events <-__mem_cgroup_commit_charge;
																						lookup_page_cgroup <-__mem_cgroup_commit_charge_lrucare;
																				__inc_zone_page_state <-add_to_page_cache_locked;
																					__inc_zone_state <-__inc_zone_page_state;
																			__lru_cache_add <-add_to_page_cache_lru;
																	alloc_page_buffers <-__getblk;
																		alloc_buffer_head <-alloc_page_buffers;
																			kmem_cache_alloc <-alloc_buffer_head;
																				should_failslab <-kmem_cache_alloc;
																			recalc_bh_state <-alloc_buffer_head;
																		set_bh_page <-alloc_page_buffers;
																	init_page_buffers.isra.10 <-__getblk;
																	unlock_page <-__getblk; // then, lock_page called before this call?
																		page_waitqueue <-unlock_page;
																	put_page <-__getblk;
																	__find_get_block <-__getblk;
																		__find_get_block_slow.isra.17 <-__find_get_block;
																			find_get_page <-__find_get_block_slow.isra.17;
																			put_page <-__find_get_block_slow.isra.17;
																		__brelse <-__find_get_block;
																		mark_page_accessed <-__find_get_block;
																bh_uptodate_or_lock <-ext4_ext_find_extent;
																bh_submit_read <-ext4_ext_find_extent;
																	submit_bh <-bh_submit_read;
																		bio_alloc <-submit_bh;
																			bio_alloc_bioset <-bio_alloc;
																				mempool_alloc <-bio_alloc_bioset;
																					mempool_alloc_slab <-mempool_alloc;
																						kmem_cache_alloc <-mempool_alloc_slab;
																							should_failslab <-kmem_cache_alloc;
																				bio_init <-bio_alloc_bioset;
																		submit_bio <-submit_bh;
																			generic_make_request <-submit_bio;
																				bio_integrity_enabled <-generic_make_request;
																				blk_throtl_bio <-generic_make_request;
																					task_blkio_cgroup <-blk_throtl_bio;
																					throtl_find_tg <-blk_throtl_bio;
																						__throtl_tg_fill_dev_details.isra.23 <-throtl_find_tg;
																					blkiocg_uupdate_dispatch_stats <-blk_throtl_bio;
																				__make_request <-generic_make_request;
																					blk_queue_bounce <-__make_request;
																					elv_merge <-__make_request;
																						elv_rqhash_find.isra.14 <-elv_merge;
																						cfq_merge <-elv_merge;
																							cfq_cic_lookup <-cfq_merge;
																					get_request_wait <-__make_request;
																						get_request <-get_request_wait;
																							elv_may_queue <-get_request;
																								cfq_may_queue <-elv_may_queue;
																									cfq_cic_lookup <-cfq_may_queue;
																							mempool_alloc <-get_request;
																								mempool_alloc_slab <-mempool_alloc;
																									kmem_cache_alloc <-mempool_alloc_slab;
																										should_failslab <-kmem_cache_alloc;
																							blk_rq_init <-get_request;
																							elv_set_request <-get_request;
																								cfq_set_request <-elv_set_request;
																									get_io_context <-cfq_set_request;
																										current_io_context <-get_io_context;
																											alloc_io_context <-current_io_context;
																												kmem_cache_alloc_node <-alloc_io_context;
																													should_failslab <-kmem_cache_alloc_node;
																									cfq_cic_lookup <-cfq_set_request;
																									kmem_cache_alloc_node <-cfq_set_request;
																										should_failslab <-kmem_cache_alloc_node;
																										// __LOOP__ { x3
																										kmem_cache_alloc <-radix_tree_preload;
																											_cond_resched <-kmem_cache_alloc;
																											should_failslab <-kmem_cache_alloc;
																										// }
																									// __LOOP__ { x2
																									_raw_spin_lock_irqsave <-cfq_set_request;
																									_raw_spin_unlock_irqrestore <-cfq_set_request;
																									// }
																									_raw_spin_lock_irqsave <-cfq_set_request;
																									cfq_get_queue <-cfq_set_request;
																										task_blkio_cgroup <-cfq_get_queue;
																										cfq_find_cfqg <-cfq_get_queue;
																										cfq_cic_lookup <-cfq_get_queue;
																											_raw_spin_lock_irqsave <-cfq_cic_lookup;
																											_raw_spin_unlock_irqrestore <-cfq_cic_lookup;
																										kmem_cache_alloc_node <-cfq_get_queue;
																											_cond_resched <-kmem_cache_alloc_node;
																											should_failslab <-kmem_cache_alloc_node;
																										_raw_spin_lock_irq <-cfq_get_queue;
																										task_blkio_cgroup <-cfq_get_queue;
																										cfq_find_cfqg <-cfq_get_queue;
																										cfq_cic_lookup <-cfq_get_queue;
																										cfq_init_prio_data <-cfq_get_queue;
																											task_nice <-cfq_init_prio_data;
																									_raw_spin_unlock_irqrestore <-cfq_set_request;
																					init_request_from_bio <-__make_request;
																						blk_rq_bio_prep <-init_request_from_bio;
																							bio_phys_segments <-blk_rq_bio_prep;
																								blk_recount_segments <-bio_phys_segments;
																									__blk_recalc_rq_segments <-blk_recount_segments;
																					cpu_coregroup_mask <-__make_request;
																					drive_stat_acct <-__make_request;
																						disk_map_sector_rcu <-drive_stat_acct;
																						part_round_stats <-drive_stat_acct;
																		bio_put <-submit_bh;
																	_cond_resched <-bh_submit_read;
																	__wait_on_buffer <-bh_submit_read;	// 6590.428529 // line:21460
																	...;

																	scsi_request_fn <-__blk_run_queue;	// line:21508

																	...;
											bio_get_nr_vecs <-do_mpage_readpage;	// line:21871
											mpage_alloc <-do_mpage_readpage;	// line:21872
												bio_alloc <-mpage_alloc;
													bio_alloc_bioset <-bio_alloc;
														mempool_alloc <-bio_alloc_bioset;
															_cond_resched <-mempool_alloc;
															...;
														bio_init <-bio_alloc_bioset;
											bio_add_page <-do_mpage_readpage;	// line:21872
												__bio_add_page.part.15 <-bio_add_page;
										put_page <-mpage_readpages;
										add_to_page_cache_lru <-mpage_readpages;	// line:21884
											add_to_page_cache_locked <-add_to_page_cache_lru;
												...;
										do_mpage_readpage <-mpage_readpages;	// line:21903
											bio_add_page <-do_mpage_readpage;	// line:21904
												...;
										put_page <-mpage_readpages;	// line:21906
										add_to_page_cache_lru <-mpage_readpages;	// line:21907
											add_to_page_cache_locked <-add_to_page_cache_lru;
												...;
										do_mpage_readpage <-mpage_readpages;	// line:21926
											bio_add_page <-do_mpage_readpage;	// line:21927
												...;
										put_page <-mpage_readpages;	// line:21949
										submit_bio <-mpage_readpages;	// line:21950
											generic_make_request <-submit_bio;
												...;
										blk_finish_plug <-mpage_readpages;	// line:21997
											blk_flush_plug_list <-blk_finish_plug;
								put_pages_list <-__do_page_cache_readahead;	// line:21999
								blk_finish_plug <-__do_page_cache_readahead;	// line:22000
									blk_flush_plug_list <-blk_finish_plug;
				find_get_page <-generic_file_aio_read;	// line:22002
				lock_page_killable <-generic_file_aio_read;	// line:22003
					...;
				unlock_page <-generic_file_aio_read;	// line:22180
					page_waitqueue <-unlock_page;
					__wake_up_bit <-unlock_page;
				mark_page_accessed <-generic_file_aio_read;	// line:22183
				file_read_actor <-generic_file_aio_read;	// line:22184
				do_page_fault <-page_fault;	// line:22185
					down_read_trylock <-do_page_fault;
					_cond_resched <-do_page_fault;
					find_vma <-do_page_fault;
					handle_mm_fault <-do_page_fault;	// line:22189
						mem_cgroup_count_vm_event <-handle_mm_fault;
							mem_cgroup_pgfault <-mem_cgroup_count_vm_event;
						handle_pte_fault <-handle_mm_fault;
							anon_vma_prepare <-handle_pte_fault;
								_cond_resched <-anon_vma_prepare;
							alloc_pages_vma <-handle_pte_fault;	// line:22195
								get_vma_policy <-alloc_pages_vma;
								policy_zonelist <-alloc_pages_vma;
								policy_nodemask <-alloc_pages_vma;
								...;
							mem_cgroup_newpage_charge <-handle_pte_fault;	// line:22208
								mem_cgroup_charge_common <-mem_cgroup_newpage_charge;
									lookup_page_cgroup <-mem_cgroup_charge_common;
									__mem_cgroup_try_charge <-mem_cgroup_charge_common;
									__mem_cgroup_commit_charge <-mem_cgroup_charge_common;
										mem_cgroup_charge_statistics.isra.27 <-__mem_cgroup_commit_charge;
										...;
										memcg_check_events <-__mem_cgroup_commit_charge;
							add_mm_counter_fast <-handle_pte_fault;
							page_add_new_anon_rmap <-handle_pte_fault;
								__inc_zone_page_state <-page_add_new_anon_rmap;
									__inc_zone_state <-__inc_zone_page_state;
								__page_set_anon_rmap <-page_add_new_anon_rmap;
								page_evictable <-page_add_new_anon_rmap;
								lru_cache_add_lru <-page_add_new_anon_rmap;
									__lru_cache_add <-lru_cache_add_lru;
							native_set_pte_at <-handle_pte_fault;
					up_read <-do_page_fault;	// line:22226
				put_page <-generic_file_aio_read;	// line:22227
				touch_atime <-generic_file_aio_read;	// line:22228
					current_fs_time <-touch_atime;
						...;
					mnt_want_write <-touch_atime;
						__mnt_is_readonly <-mnt_want_write;
					__mark_inode_dirty <-touch_atime;
						ext4_dirty_inode <-__mark_inode_dirty;
							ext4_journal_start_sb <-ext4_dirty_inode;
								jbd2_journal_start <-ext4_journal_start_sb;
									jbd2__journal_start <-jbd2_journal_start;
										kmem_cache_alloc <-jbd2__journal_start;
											...;
										start_this_handle.isra.9 <-jbd2__journal_start;
											kmem_cache_alloc_trace <-start_this_handle.isra.9;	// line:22242
											...;
											__jbd2_log_space_left <-start_this_handle.isra.9;	// line:22257
											kfree <-start_this_handle.isra.9;	// line:22258
							ext4_mark_inode_dirty <-ext4_dirty_inode;	// line:22259
								_cond_resched <-ext4_mark_inode_dirty;
								ext4_reserve_inode_write <-ext4_mark_inode_dirty;
									ext4_get_inode_loc <-ext4_reserve_inode_write;
										__ext4_get_inode_loc <-ext4_get_inode_loc;
											ext4_get_group_desc <-__ext4_get_inode_loc;
											ext4_inode_table <-__ext4_get_inode_loc;
											__getblk <-__ext4_get_inode_loc;
												__find_get_block <-__getblk;
													__find_get_block_slow.isra.17 <-__find_get_block;
														find_get_page <-__find_get_block_slow.isra.17;	// line:22269
														...;
				blk_finish_plug <-generic_file_aio_read;	// line:22562
					blk_flush_plug_list <-blk_finish_plug;
		__fsnotify_parent <-vfs_read;
		fsnotify <-vfs_read;	// line:22565
	sys_write <-system_call_fastpath;	// line:22566
		fget_light <-sys_write;
		vfs_write <-sys_write;	// line:22568
			rw_verify_area <-vfs_write;
				security_file_permission <-rw_verify_area;

};
//} original (unmodified)




##------------------------------------------------------------------------
##{kernel configuration

General setup --->
	<M> Kernel .config support # /proc/config.gz
	[*]	Enable access to .config through /proc/config.gz
	-*- Control Group support [ALL]
	[*] Enable VM event counters for /proc/vmstat
	[*] Disable heap randomization
	<M> OProfile system profiling
Device Drivers --->
	[*] Multiple devices driver support (RAID and LVM) --->
		<M> Cache target support (EXPERIMENTAL)
File systems --->
	<*> The Extended 4 (ext4) filesystem
	[*]	Ext4 debugging support
Kernel hacking --->
	[*] Fault-injection framework
	[*]	Fault-injection capability for kmalloc
	[*]	Fault-injection capability for alloc_pages()
	[*]	Fault-injection capability for disk IO
	[*]	Fault-injection capability for faking disk interrupts
	[*]	Debugfs entries for fault-injection capabilities
	[*] Latency measuring infrastructure
	[*] Tracers --->
	[*] Enable dynamic printk() support
	[*] Export kernel pagetable layout to userspace via debugfs
Security options --->
	[*] AppArmor support
	(1)	AppArmor boot parameter default value
	Default security module (AppArmor)

##}kernel configuration








#if 1 /* { BLUSJUNE_CODE_ZONE_OPEN */
#endif /* } BLUSJUNE_CODE_ZONE_CLOSE */




//
//
//
//} FREENOTE #--------------------------------------------------------------




:: arch/x86/include/asm/page_types.h:
#define PAGE_SHIFT		12




:: include/linux/mm_types.h:
//{

/*
 * Each physical page in the system has a struct page associated with
 * it to keep track of whatever it is we are using the page for at the
 * moment. Note that we have no way to track which tasks are using
 * a page, though if it is a pagecache page, rmap structures can tell us
 * who is mapping it.
 */
struct page {
	unsigned long flags;		/* Atomic flags, some possibly
					 * updated asynchronously */
	atomic_t _count;		/* Usage count, see below. */
	union {
		atomic_t _mapcount;	/* Count of ptes mapped in mms,
					 * to show when page is mapped
					 * & limit reverse map searches.
					 */
		struct {		/* SLUB */
			u16 inuse;
			u16 objects;
		};
	};
	union {
	    struct {
		unsigned long private;		/* Mapping-private opaque data:
					 	 * usually used for buffer_heads
						 * if PagePrivate set; used for
						 * swp_entry_t if PageSwapCache;
						 * indicates order in the buddy
						 * system if PG_buddy is set.
						 */
		struct address_space *mapping;	/* If low bit clear, points to
						 * inode address_space, or NULL.
						 * If page mapped as anonymous
						 * memory, low bit is set, and
						 * it points to anon_vma object:
						 * see PAGE_MAPPING_ANON below.
						 */
	    };
#if USE_SPLIT_PTLOCKS
	    spinlock_t ptl;
#endif
	    struct kmem_cache *slab;	/* SLUB: Pointer to slab */
	    struct page *first_page;	/* Compound tail pages */
	};
	union {
		pgoff_t index;		/* Our offset within mapping. */
		void *freelist;		/* SLUB: freelist req. slab lock */
	};
	struct list_head lru;		/* Pageout list, eg. active_list
					 * protected by zone->lru_lock !
					 */
	/*
	 * On machines where all RAM is mapped into kernel address space,
	 * we can simply calculate the virtual address. On machines with
	 * highmem some memory is mapped into kernel virtual memory
	 * dynamically, so we need a place to store that address.
	 * Note that this field could be 16 bits on x86 ... ;)
	 *
	 * Architectures with slow multiplication can define
	 * WANT_PAGE_VIRTUAL in asm/page.h
	 */
#if defined(WANT_PAGE_VIRTUAL)
	void *virtual;			/* Kernel virtual address (NULL if
					   not kmapped, ie. highmem) */
#endif /* WANT_PAGE_VIRTUAL */
#ifdef CONFIG_WANT_PAGE_DEBUG_FLAGS
	unsigned long debug_flags;	/* Use atomic bitops on this */
#endif

#ifdef CONFIG_KMEMCHECK
	/*
	 * kmemcheck wants to track the status of each byte in a page; this
	 * is a pointer to such a status block. NULL if not tracked.
	 */
	void *shadow;
#endif
};


//}




block/blk-core.c: void submit_bio(int rw, struct bio *bio)
{
	int count = bio_sectors(bio);

	bio->bi_rw |= rw;

	if (bio_has_data(bio) && !(rw & REQ_DISCARD)) {
		if (rw & WRITE) {
			count_vm_events(PGPGOUT, count);	// collecting stat info
		} else {
			task_io_account_read(bio->bi_size);	// collecting stat info
			count_vm_events(PGPGIN, count);		// collecting stat info
		}

		if (unlikely(block_dump)) {
			char b[BDEVNAME_SIZE];
			printk(KERN_DEBUG
"%s(%d): %s block %Lu on %s (%u sectors)\n",
current->comm,
task_pid_nr(current),
(rw & WRITE) ? "WRITE" : "READ",
(unsigned long long)bio->bi_sector,
bdevname(bio->bi_bdev, b),
count
);
		}
	}

	generic_make_request(bio);
} // submit_bio()




//{ :: include/linux/bio.h:

/*
 * various member access, note that bio_data should of course not be used
 * on highmem page vectors
 */
#define bio_iovec_idx(bio, idx)	(&((bio)->bi_io_vec[(idx)]))
#define bio_iovec(bio)		bio_iovec_idx((bio), (bio)->bi_idx)
#define bio_page(bio)		bio_iovec((bio))->bv_page
#define bio_offset(bio)		bio_iovec((bio))->bv_offset
#define bio_segments(bio)	((bio)->bi_vcnt - (bio)->bi_idx)
#define bio_sectors(bio)	((bio)->bi_size >> 9)

//} :: include/linux/bio.h:




//{ :: include/linux/blk_types.h:


/*
 * main unit of I/O for the block layer and lower layers (ie drivers and
 * stacking drivers)
 */
struct bio {
	sector_t		bi_sector;	/* device address in 512 byte
						   sectors */
	struct bio		*bi_next;	/* request queue link */
	struct block_device	*bi_bdev;
	unsigned long		bi_flags;	/* status, command, etc */
	unsigned long		bi_rw;		/* bottom bits READ/WRITE,
						 * top bits priority
						 */

	unsigned short		bi_vcnt;	/* how many bio_vec's */
	unsigned short		bi_idx;		/* current index into bvl_vec */

	/* Number of segments in this BIO after
	 * physical address coalescing is performed.
	 */
	unsigned int		bi_phys_segments;

	unsigned int		bi_size;	/* residual I/O count */

	/*
	 * To keep track of the max segment size, we account for the
	 * sizes of the first and last mergeable segments in this bio.
	 */
	unsigned int		bi_seg_front_size;
	unsigned int		bi_seg_back_size;

	unsigned int		bi_max_vecs;	/* max bvl_vecs we can hold */

	unsigned int		bi_comp_cpu;	/* completion CPU */

	atomic_t		bi_cnt;		/* pin count */

	struct bio_vec		*bi_io_vec;	/* the actual vec list */

	bio_end_io_t		*bi_end_io;

	void			*bi_private;
#if defined(CONFIG_BLK_DEV_INTEGRITY)
	struct bio_integrity_payload *bi_integrity;  /* data integrity */
#endif

	bio_destructor_t	*bi_destructor;	/* destructor */

	/*
	 * We can inline a number of vecs at the end of the bio, to avoid
	 * double allocations for a small number of bio_vecs. This member
	 * MUST obviously be kept at the very end of the bio.
	 */
	struct bio_vec		bi_inline_vecs[0];
};




//} :: include/linux/blk_types.h:


include/linux/fs.h:
/* fs/block_dev.c */
#define BDEVNAME_SIZE	32	/* Largest string for a blockdev identifier */
#define BDEVT_SIZE	10	/* Largest string for MAJ:MIN for blkdev */





//{ :: fs/partitions/check.c: 

/*
 * disk_name() is used by partition check code and the genhd driver.
 * It formats the devicename of the indicated disk into
 * the supplied buffer (of size at least 32), and returns
 * a pointer to that same buffer (for convenience).
 */

char *disk_name(struct gendisk *hd, int partno, char *buf)
{
	if (!partno)
		snprintf(buf, BDEVNAME_SIZE, "%s", hd->disk_name);
	else if (isdigit(hd->disk_name[strlen(hd->disk_name)-1]))
		snprintf(buf, BDEVNAME_SIZE, "%sp%d", hd->disk_name, partno);
	else
		snprintf(buf, BDEVNAME_SIZE, "%s%d", hd->disk_name, partno);

	return buf;
}

const char *bdevname(struct block_device *bdev, char *buf)
{
	return disk_name(bdev->bd_disk, bdev->bd_part->partno, buf);
}

//} :: fs/partitions/check.c: 




include/linux/task_io_accounting_ops.h: static inline void task_io_account_read(size_t bytes)
{
#ifdef CONFIG_TASK_IO_ACCOUNTING
	current->ioac.read_bytes += bytes;
#else
	/* emtpy */
#endif
}




:: mm/filemap.c:
//{

// 20121206_174754[] MUST check what arguments are passed into do_generic_file_read() function: to know the real value of *ppos
// mm/filemap.c:
// generic_file_aio_read(struct kiocb *iocb, ...)
// {
// 	loff_t *ppos = &iocb->ki_pos;
// 	...;
// 	do_generic_file_read(filp, ppos, &desc, file_read_actor);
// 	retval += desc.written;
// 	...;
// }
//
// include/linux/aio.h:
// struct kiocb {
// 	...;
// 	loff_t ki_pos;
// 	...;
// }
//
@ static void do_generic_file_read(
		struct file *filp,
		loff_t *ppos,
		read_descriptor_t *desc, // defined as a structure in "include/linux/fs.h"
		read_actor_t actor
		)
{
	...;

	// BLUSJUNE_NOTE#20121206_175833
	//
	// [ definition of 'index' and 'offset' ]
	//
	// index: data location represented as the number of pages
	// 	((" >> PAGE_CACHE_SHIFT"))
	//
	// offset: data location inside a page
	// 	((" & ~PAGE_CACHE_MASK"))
	//
	// Thus we can find the location of data
	// by using 'index' - to pick a corresponding page
	// and then using 'offset' - to know the start position of the data inside the page
	//
	pgoff_t index;	// index = *ppos >> PAGE_CACHE_SHIFT;
	pgoff_t last_index;
	pgoff_t prev_index;
	unsigned long offset;	// offset = *ppos & ~PAGE_CACHE_MASK;
	unsigned int prev_offset;

	...;

	for (;;) {
		struct page * page;
		pgoff_t end_index;
		loff_t isize;
		unsigned long nr, ret;

find_page:
		page = find_get_page(mapping, index);

		// try to read ahead if page is not found
		if (!page) {
			page_cache_sync_readahead(...);
			page = find_get_page(mapping, index);
			if (unlikely(page == NULL))
				goto no_cached_page;
		}

		// try once more to find a page in async mode?
		//
		// BLUSJUNE_QUESTION#20121204_112144
		// what's the reason to re-execute
		// async version of readahead after executing readahead once?
		// >> to guarantee the safety?
		if (PageReadahead(page)) {
			page_cache_async_readahead(...);
		}
		if (!PageUptodate(page)) {
			if (inote->i_blkbits == PAGE_CACHE_SHIFT ||
					!mapping->a_ops->is_partially_uptodate)
				goto page_not_up_to_date;
			if (!trylock_page(page))
				goto page_not_up_to_date;
			// Did it get truncated before we got the lock?
			if (!page->mapping)
				goto page_not_up_to_date_locked;
			if (!mapping->a_ops->is_partially_uptodate(page, desc, offset))
				goto page_not_up_to_date_locked;
				// mapping->a_ops->is_partially_uptodate()
				// may be actually block_is_partially_uptodate(),
				// which is defined as a normal function in fs/buffers.c
			unlock_page(page);
		}
page_ok:
		...;
		if (unlikely(!isize || index > end_index)) {
			page_cache_release(page);
			goto out;
		}

		// 'nr' is the maximum number of bytes to copy from this page
		...;
		nr = PAGE_CACHE_SIZE;
		if (index == end_index) {
			nr = ...;
			if (nr <= offset) {
				page_cache_release(page);
				goto out;
			}
		}
		nr = nr - offset;

		/* take care about potential aliasing before reading the page on the kernel side */
		if (mapping_writably_mapped(mapping))
			flush_dcache_page(page);

		/*
		 * when a sequential read accesses a page several times,
		 * only mark it as accessed the first time.
		 */
		if (prev_index != index || offset != prev_offset)
			mark_page_accessed(page);
		prev_index = index;


		/*
		 * Ok, now we have the page, and it's up-to-date,
		 * so now we can copy it to user space...
		 *
		 * NOTE! this may not be the same
		 * as how much of a user buffer we filled up (we may be padding etc)
		 * so we can only update "pos" here
		 * (the actor routine has to update the user buffer pointers
		 * and the remaining count).
		 */
		// 'actor' here actually is 'file_read_actor()' defined as a real function in mm/filemap.c
		// evidence:
		// 	generic_file_aio_read() function calls
		// 	do_generic_file_read() with the following parameters:
		// 	do_generic_file_read(filp, ppos, &desc, file_read_actor);
		//
		// 'actor' returns the number of bytes actually processed
		ret = actor(desc, page, offset, nr);

		// offset 
		//
		offset += ret;
		index += offset >> PAGE_CACHE_SHIFT;
		offset &= ~PAGE_CACHE_MASK;
		prev_offset = offset;

		page_cache_release(page);
		// BLUSJUNE_QUESTION#20121204_140928
		// what's the role/function of page_cache_release()?

		if (ret == nr && desc->count)
			continue;
		goto out;

page_not_up_to_date:
		// 




page_not_up_to_date_locked:



readpage:



readpage_error:
		// UHHHHH! a synchronous read error occurred. Report it!
		desc->error = error;
		page_cache_release(page);
		goto out; // bad case!!!

no_cached_page: // yes, it wasn't cached yet. so we need to create a new page
		page = page_cache_alloc_cold(mapping); // defined as inline function in "include/linux/pagemap.h"
		if (!page) {
			desc->error = -ENOMEM;
			goto out; // bad case ??
		}
		error = add_to_page_cache_lru(page, mapping, index, GFP_KERNEL);
		if (error) {
			page_cache_release(page);
			if (error == -EEXIST)
				goto find_page; // not so bad, try again to find a corresponding page
			desc->error = error;
			goto out; // bad case ??
		}
		goto readpage;
	}







out: // final out path from this function
	ra->prev_pos = prev_index;
	ra->prev_pos <<= PAGE_CACHE_SHIFT;
	ra->prev_pos |= prev_offset;
	*ppos = ((loff_t)index << PAGE_CACHE_SHIFT) + offset;
	file_accessed(filp);	// defined as inline function in "include/linux/fs.h"
}







@ struct page * find_get_page(struct address_space *mapping, pgoff_t offset)
{
	void **pagep;
	struct page *page;

repeat:
	page = NULL;
	pagep = radix_tree_lookup_slot(&mapping->page_tree, offset);
	if (pagep) {
		page = radix_tree_dref_slot(pagep);
	}
}




@ add_to_page_cache_lru()
{
	ret = add_to_page_cache(...);
	if (ret == 0)
		lru_cache_add_file(page);
	return ret;
}

@ add_to_page_cache_locked()
{
// this function is real & final core routine
//	add_to_page_cache() directly calls this function
//	add_to_page_cache_lru() indirectly calls this function
// to add a page to the pagecache - it must be locked
// it does not add the page to the LRU - the caller must do that
// add_to_page_cache_lru() function eventually calls this through add_to_page_cache()
}




@ struct page * find_or_create_page(
		struct address_space *mapping,
		pgoff_t index,
		gfp_t gfp_mask)
{
// locates a page in the pagecache
// if the page is not present,
//	a new page is allocagted using @gfp_mask
//	and is added to the pagecache
//	and is added to the VM's LRU list
// the returned page
//	is locked
//	and has its reference count incremented
// this function returns
//	the desired page's address
//	or zero on memory exhaustion

	page = find_lock_page();
	if (!page) {
		page = __page_cache_alloc(gfp_mask);
		if (!page)
			return NULL;
		err = add_to_page_cache_lru(...);
		if (unlikely(err)) {
			page_cache_release(page);
			...
		}
	}
	return page;
}




int file_read_actor(read_descriptor_t *desc, struct page *page,
			unsigned long offset, unsigned long size)
{
	char * kaddr;
	unsigned long left, count = desc->count;

	if (size > count)
		size = count;

	//   Check the faults on the destination side of a read:
	//   Because faults on the destination of a read are common,
	// so do it before taking the kmap.
	if (!fault_in_pages


}




//}




:: include/linux/buffer_head.h:
//{

// the following macros are defined here by the BUFFER_FNS meta macro:
//
//	buffer_uptodate()
//	buffer_dirty()
//	buffer_locked()
//	buffer_req()
//	buffer_mapped()
//	buffer_new()
//	buffer_async_read()
//	buffer_async_write()
//	buffer_delay()
//	buffer_boundary()
//	buffer_write_io_error()
//	buffer_unwritten()


/*
 * macro tricks to expand the set_buffer_foo(), clear_buffer_foo()
 * and buffer_foo() functions.
 */
#define BUFFER_FNS(bit, name)                                           \
static inline void set_buffer_##name(struct buffer_head *bh)            \
{                                                                       \
        set_bit(BH_##bit, &(bh)->b_state);                              \
}                                                                       \
static inline void clear_buffer_##name(struct buffer_head *bh)          \
{                                                                       \
        clear_bit(BH_##bit, &(bh)->b_state);                            \
}                                                                       \
static inline int buffer_##name(const struct buffer_head *bh)           \
{                                                                       \
        return test_bit(BH_##bit, &(bh)->b_state);                      \
}

/*
 * test_set_buffer_foo() and test_clear_buffer_foo()
 */
#define TAS_BUFFER_FNS(bit, name)                                       \
static inline int test_set_buffer_##name(struct buffer_head *bh)        \
{                                                                       \
        return test_and_set_bit(BH_##bit, &(bh)->b_state);              \
}                                                                       \
static inline int test_clear_buffer_##name(struct buffer_head *bh)      \
{                                                                       \
        return test_and_clear_bit(BH_##bit, &(bh)->b_state);            \
}                                                                       \

/*
 * Emit the buffer bitops functions.   Note that there are also functions
 * of the form "mark_buffer_foo()".  These are higher-level functions which
 * do something in addition to setting a b_state bit.
 */
BUFFER_FNS(Uptodate, uptodate)
BUFFER_FNS(Dirty, dirty)
TAS_BUFFER_FNS(Dirty, dirty)
BUFFER_FNS(Lock, locked)
BUFFER_FNS(Req, req)
TAS_BUFFER_FNS(Req, req)
BUFFER_FNS(Mapped, mapped)
BUFFER_FNS(New, new)
BUFFER_FNS(Async_Read, async_read)
BUFFER_FNS(Async_Write, async_write)
BUFFER_FNS(Delay, delay)
BUFFER_FNS(Boundary, boundary)
BUFFER_FNS(Write_EIO, write_io_error)
BUFFER_FNS(Unwritten, unwritten)





enum bh_state_bits {
        BH_Uptodate,    /* Contains valid data */
        BH_Dirty,       /* Is dirty */
        BH_Lock,        /* Is locked */
        BH_Req,         /* Has been submitted for I/O */
        BH_Uptodate_Lock,/* Used by the first bh in a page, to serialise
                          * IO completion of other buffers in the page
                          *                           */

        BH_Mapped,      /* Has a disk mapping */
        BH_New,         /* Disk mapping was newly created by get_block */
        BH_Async_Read,  /* Is under end_buffer_async_read I/O */
        BH_Async_Write, /* Is under end_buffer_async_write I/O */
        BH_Delay,       /* Buffer is not yet allocated on disk */
        BH_Boundary,    /* Block is followed by a discontiguity */
        BH_Write_EIO,   /* I/O error on write */
        BH_Unwritten,   /* Buffer is allocated on disk but not written */
        BH_Quiet,       /* Buffer Error Prinks to be quiet */

        BH_PrivateStart,/* not a state bit, but the first bit available
                         * for private allocation by other entities
                         *                          */
};




// struct buffer_head * bh;
// bh->b_blocknr;	// (sector_t) // unsigned long or u64
// bh->b_size;		// (size_t) // unsigned long
// bh->b_data;		// (char *)
//
struct buffer_head {
	unsigned long b_state;			// buffer state bitmap
	struct buffer_head *b_this_page;	// circular list of page's buffers
	struct page *b_page;			// the page this bh is mapped to

	sector_t b_blocknr;			// start block number
	size_t b_size;				// size of mapping
	char *b_data;				// pointer to data within the page

	struct block_device *b_bdev;
	bh_end_io_t *b_end_io;			// I/O completion
	void *b_private;			// reserved for b_end_io
	struct list_head b_assoc_buffers;	// associated with another mapping
	struct address_space *b_assoc_map;	// mapping this buffer is associated with
	atomic_t b_count;			// users using this buffer_head
};




//} :: include/linux/buffer_head.h:




:: include/linux/types.h:
//{


typedef struct {
	int counter;
} atomic_t;


#ifndef pgoff_t
#define pgoff_t unsigned long
#endif /* pgoff_t */


// the type used for indexing onto a disk or disk partition.
//
// Linux always considers sectors to be 512 bytes long independently
// of the devices real block size
//
// blkcnt_t is the type of the inode's block count
#ifdef CONFIG_LBDAF
typedef u64 sector_t;
typedef u64 blkcnt_t;
#else
typedef unsigned long sector_t;
typedef unsigned long blkcnt_t;
#endif




//} :: include/linux/types.h:




:: fs/buffer.c:
//{

sector_t generic_block_bmap(struct address_space *mapping, sector_t block, get_block_t *get_block)
{
	struct buffer_head tmp;
	struct inode *inode = mapping->host;
	tmp.b_state = 0;
	tmp.b_blocknr = 0;
	tmp.b_size = 1 << inode->i_blkbits;
	get_block(inode, block, &tmp, 0);
	return tmp.b_blocknr;
}



/*
 * block_is_partially_uptodate checks whether buffers within a page are
 * uptodate or not.
 *
 * Returns true if all buffers which correspond to a file portion
 * we want to read are uptodate.
 */
int block_is_partially_uptodate(struct page *page, read_descriptor_t *desc,
                                        unsigned long from)
{
	...
	int ret = 1;


	if (!page_has_buffers(page))
		return 0;	// returns '0' if page does not have buffers
	blocksize = 1 << inote->i_blkbits;
	to = min_t(unsigned, PAGE_CACHE_SIZE - from, desc->count);
	to = from + to;
	if (from < blocksize && to > PAGE_CACHE_SIZE - blocksize)
		return 0;	// returns '0' if 'from' to 'to' range is not proper

	head = page_buffers(page);
	bh = head;
	block_start = 0;
	do {
		block_end = block_start + blocksize;
		if (block_end > from && block_start < to) {
			if (!buffer_uptodate(bh)) {
				ret = 0;
				break;
			}
			if (block_end >= to)
				break;
		}
		block_start = block_end;
		bh = bh->b_this_page;
	} while (bh != head);
}


//}





:: include/linux/pagemap.h:
//{

#define PAGE_CACHE_SHIFT	PAGE_SHIFT

static inline struct page *page_cache_alloc(struct address_space *x)
{
        return __page_cache_alloc(mapping_gfp_mask(x));
}

static inline struct page *page_cache_alloc_cold(struct address_space *x)
{
        return __page_cache_alloc(mapping_gfp_mask(x)|__GFP_COLD);
}

static inline struct page *page_cache_alloc_readahead(struct address_space *x)
{
        return __page_cache_alloc(mapping_gfp_mask(x) |
                                  __GFP_COLD | __GFP_NORETRY | __GFP_NOWARN);
}


@ static inline int add_to_page_cache(...)
{
	__set_page_locked(page);
	add_to_page_cache_locked(...);
}
//} :: include/linux/pagemap.h:




:: lib/radix-tree.c:
//{
#ifdef __KERNEL__
#define RADIX_TREE_MAP_SHIFT    (CONFIG_BASE_SMALL ? 4 : 6)
#else
#define RADIX_TREE_MAP_SHIFT    3       /* For more stressful testing */
#endif

#define RADIX_TREE_MAP_SIZE     (1UL << RADIX_TREE_MAP_SHIFT)
#define RADIX_TREE_MAP_MASK     (RADIX_TREE_MAP_SIZE-1)




struct radix_tree_node {
	unsigned int	height;
	unsigned int	count;
	struct rcu_head	rcu_head;
	void __rcu	*slots[RADIX_TREE_MAP_SIZE];
	unsigned long	tags[RADIX_TREE_MAX_TAGS][RADIX_TREE_TAG_LONGS];
};



/*
 * is_slot == 1 : search for the slot.
 * is_slot == 0 : search for the node.
 */
static void *radix_tree_lookup_element(struct radix_tree_root *root,
                                unsigned long index, int is_slot)
{
	unsigned int height, shift;
	struct radix_tree_node *node, **slot;

	node = rcu_dereference_raw(root->rnode);
	if (node == NULL)
		return NULL;

	...

        shift = (height-1) * RADIX_TREE_MAP_SHIFT;

        do {
                slot = (struct radix_tree_node **)
                        (node->slots + ((index>>shift) & RADIX_TREE_MAP_MASK));
                node = rcu_dereference_raw(*slot);
                if (node == NULL)
                        return NULL;

                shift -= RADIX_TREE_MAP_SHIFT;
                height--;
        } while (height > 0);

        return is_slot ? (void *)slot : indirect_to_ptr(node);

}




void **radix_tree_lookup_slot(struct radix_tree_root *root, unsigned long index)
{
	return (void **)radix_tree_lookup_element(root, index, 1);
}




//} :: lib/radix-tree.c:



:: include/linux/rcupdate.h:
//{

// RCU (Read-Copy Update) mechanism for mutual exclusion
@ static inline void rcu_read_lock(void)
@ static inline void rcu_read_unlock(void)
//}




:: fs/read_write.c:
//{

ssize_t vfs_read(struct file *file, char __user *buf, size_t count, loff_t *pos)
{
	...;
	ret = rw_verify_area(READ, file, pos, count);
	if (ret >= 0) {
		count = ret;
		if (file->f_op->read)
			ret = file->f_op->read(file, buf, count, pos);
		else
			ret = do_sync_read(file, buf, count, pos);
		if (ret > 0) {
			...;
		}
		inc_syscr(current);
	}
	return ret;
}
//} :: fs/read_write.c:




:: include/linux/fs.h:
//{

struct file {
	/*
	 * fu_list becomes invalid after file_free is called and queued via
	 * fu_rcuhead for RCU freeing
	 */
	union {
		struct list_head	fu_list;
		struct rcu_head 	fu_rcuhead;
	} f_u;
	struct path		f_path;
#define f_dentry	f_path.dentry
#define f_vfsmnt	f_path.mnt
	const struct file_operations	*f_op;
	spinlock_t		f_lock;  /* f_ep_links, f_flags, no IRQ */
#ifdef CONFIG_SMP
	int			f_sb_list_cpu;
#endif
	atomic_long_t		f_count;
	unsigned int 		f_flags;
	fmode_t			f_mode;
	loff_t			f_pos;
	struct fown_struct	f_owner;
	const struct cred	*f_cred;
	struct file_ra_state	f_ra;

	u64			f_version;
#ifdef CONFIG_SECURITY
	void			*f_security;
#endif
	/* needed for tty driver, and maybe others */
	void			*private_data;

#ifdef CONFIG_EPOLL
	/* Used by fs/eventpoll.c to link all the hooks to this file */
	struct list_head	f_ep_links;
#endif /* #ifdef CONFIG_EPOLL */
	struct address_space	*f_mapping;
#ifdef CONFIG_DEBUG_WRITECOUNT
	unsigned long f_mnt_write_state;
#endif
};




/* track a single file's readahead state */
struct file_ra_state {
	pgoff_t start;			/* where readahead started */
	unsigned int size;		/* # of readahead pages */
	unsigned int aysnc_size;	/* do asynchronous readahead
					   when there are only # of pages ahead */
	unsigned int ra_pages;		/* maximum readahead window */
	unsigned int mmap_miss;		/* cache miss stat for mmap accesses */
	loff_t prev_pos;		/* cache last read() position */
}; /* struct file_ra_state {} */




struct inode {
	/* RCU path lookup touches following: */
	umode_t			i_mode;
	uid_t			i_uid;
	gid_t			i_gid;
	const struct inode_operations	*i_op;
	struct super_block	*i_sb;

	spinlock_t		i_lock;	/* i_blocks, i_bytes, maybe i_size */
	unsigned int		i_flags;
	unsigned long		i_state;
#ifdef CONFIG_SECURITY
	void			*i_security;
#endif
	struct mutex		i_mutex;


	unsigned long		dirtied_when;	/* jiffies of first dirtying */

	struct hlist_node	i_hash;
	struct list_head	i_wb_list;	/* backing dev IO list */
	struct list_head	i_lru;		/* inode LRU list */
	struct list_head	i_sb_list;
	union {
		struct list_head	i_dentry;
		struct rcu_head		i_rcu;
	};
	unsigned long		i_ino;
	atomic_t		i_count;
	unsigned int		i_nlink;
	dev_t			i_rdev;
	unsigned int		i_blkbits;
	u64			i_version;
	loff_t			i_size;
#ifdef __NEED_I_SIZE_ORDERED
	seqcount_t		i_size_seqcount;
#endif
	struct timespec		i_atime;
	struct timespec		i_mtime;
	struct timespec		i_ctime;
	blkcnt_t		i_blocks;
	unsigned short          i_bytes;
	struct rw_semaphore	i_alloc_sem;
	const struct file_operations	*i_fop;	/* former ->i_op->default_file_ops */
	struct file_lock	*i_flock;
	struct address_space	*i_mapping;
	struct address_space	i_data;
#ifdef CONFIG_QUOTA
	struct dquot		*i_dquot[MAXQUOTAS];
#endif
	struct list_head	i_devices;
	union {
		struct pipe_inode_info	*i_pipe;
		struct block_device	*i_bdev;
		struct cdev		*i_cdev;
	};

	__u32			i_generation;

#ifdef CONFIG_FSNOTIFY
	__u32			i_fsnotify_mask; /* all events this inode cares about */
	struct hlist_head	i_fsnotify_marks;
#endif

#ifdef CONFIG_IMA
	atomic_t		i_readcount; /* struct files open RO */
#endif
	atomic_t		i_writecount;
#ifdef CONFIG_FS_POSIX_ACL
	struct posix_acl	*i_acl;
	struct posix_acl	*i_default_acl;
#endif
	void			*i_private; /* fs or device private pointer */
};




struct address_space {
	struct inode		*host;		/* owner: inode, block_device */
	struct radix_tree_root	page_tree;	/* radix tree of all pages */
	spinlock_t		tree_lock;	/* and lock protecting it */
	unsigned int		i_mmap_writable;/* count VM_SHARED mappings */
	struct prio_tree_root	i_mmap;		/* tree of private and shared mappings */
	struct list_head	i_mmap_nonlinear;/*list VM_NONLINEAR mappings */
	struct mutex		i_mmap_mutex;	/* protect tree, count, list */
	/* Protected by tree_lock together with the radix tree */
	unsigned long		nrpages;	/* number of total pages */
	pgoff_t			writeback_index;/* writeback starts here */
	const struct address_space_operations *a_ops;	/* methods */
	unsigned long		flags;		/* error bits/gfp mask */
	struct backing_dev_info *backing_dev_info; /* device readahead, etc */
	spinlock_t		private_lock;	/* for use by the address_space */
	struct list_head	private_list;	/* ditto */
	struct address_space	*assoc_mapping;	/* ditto */
} __attribute__((aligned(sizeof(long))));




/*
 * "descriptor" for what we're up to with a read.
 * This allows us to use the same read code yet
 * have multiple different users of the data that
 * we read from a file.
 *
 * The simplest case just copies the data to user
 * mode.
 */
typedef struct {
	size_t written;
	size_t count;
	union {
		char __user *buf;
		void *data;
	} arg;
	int error;
} read_descriptor_t;




typedef int (*read_actor_t)(read_descriptor_t *, struct page *,
		unsigned long, unsigned long);




struct address_space_operations {
	int (*writepage)(struct page *page, struct writeback_control *wbc);
	int (*readpage)(struct file *, struct page *);

	/* Write back some dirty pages from this mapping. */
	int (*writepages)(struct address_space *, struct writeback_control *);

	/* Set a page dirty.  Return true if this dirtied it */
	int (*set_page_dirty)(struct page *page);

	int (*readpages)(struct file *filp, struct address_space *mapping,
			struct list_head *pages, unsigned nr_pages);

	int (*write_begin)(struct file *, struct address_space *mapping,
				loff_t pos, unsigned len, unsigned flags,
				struct page **pagep, void **fsdata);
	int (*write_end)(struct file *, struct address_space *mapping,
				loff_t pos, unsigned len, unsigned copied,
				struct page *page, void *fsdata);

	/* Unfortunately this kludge is needed for FIBMAP. Don't use it */
	sector_t (*bmap)(struct address_space *, sector_t);
	void (*invalidatepage) (struct page *, unsigned long);
	int (*releasepage) (struct page *, gfp_t);
	void (*freepage)(struct page *);
	ssize_t (*direct_IO)(int, struct kiocb *, const struct iovec *iov,
			loff_t offset, unsigned long nr_segs);
	int (*get_xip_mem)(struct address_space *, pgoff_t, int,
						void **, unsigned long *);
	/* migrate the contents of a page to the specified target */
	int (*migratepage) (struct address_space *,
			struct page *, struct page *);
	int (*launder_page) (struct page *);
	int (*is_partially_uptodate) (struct page *, read_descriptor_t *,
					unsigned long);
	int (*error_remove_page)(struct address_space *, struct page *);
};




typedef int (*read_actor_t)(read_descriptor_t *, struct page *, unsigned long, unsigned long);

extern int file_read_actor(read_descriptor_t * desc, struct page *page, unsigned long offset, unsigned long size);
// file_read_actor() is actually a 'read_actor_t' type function




extern void touch_atime(struct vfsmount *mnt, struct dentry *dentry);
static inline void file_accessed(struct file *file)
{      
        if (!(file->f_flags & O_NOATIME))
                touch_atime(file->f_path.mnt, file->f_path.dentry);
}




struct block_device {
	dev_t			bd_dev;		// not a kdev_td - it's a search key
	int			bd_openers;
	struct inode *		bd_inode;
	struct super_block *	bd_super;
	struct mutex		bd_mutex;
	struct list_head	bd_inodes;
	void *			bd_claiming;
	void *			bd_holder;
	int			bd_holders;
	bool			bd_write_holder;
#ifdef CONFIG_SYSFS
	struct list_head	bd_holder_disks;
#endif
	struct block_device *	bd_contains;
	unsigned		bd_block_size;
	struct hd_struct *	bd_part;	// genhd: generic hard disk
	unsigned		bd_part_count;
	int			bd_invalidated;
	struct gendisk *	bd_disk;
	struct list_head	bd_list;
	unsigned long		bd_private;
	int			bd_fsfreeze_count;
	struct mutex		bd_fsfreeze_mutex;
}; // struct block_device





//} :: include/linux/fs.h:




:: include/linux/dcache.h:
//{

struct dentry {
	/* RCU lookup touched fields */
	unsigned int d_flags;		/* protected by d_lock */
	seqcount_t d_seq;		/* per dentry seqlock */
	struct hlist_bl_node d_hash;	/* lookup hash list */
	struct dentry *d_parent;	/* parent directory */
	struct qstr d_name;
	struct inode *d_inode;		/* Where the name belongs to - NULL is
					 * negative */
	unsigned char d_iname[DNAME_INLINE_LEN];	/* small names */

	/* Ref lookup also touches following */
	unsigned int d_count;		/* protected by d_lock */
	spinlock_t d_lock;		/* per dentry lock */
	const struct dentry_operations *d_op;
	struct super_block *d_sb;	/* The root of the dentry tree */
	unsigned long d_time;		/* used by d_revalidate */
	void *d_fsdata;			/* fs-specific data */

	struct list_head d_lru;		/* LRU list */
	/*
	 * d_child and d_rcu can share memory
	 */
	union {
		struct list_head d_child;	/* child of parent list */
	 	struct rcu_head d_rcu;
	} d_u;
	struct list_head d_subdirs;	/* our children */
	struct list_head d_alias;	/* inode alias list */
};




struct qstr {
	unsigned int hash;
	unsigned int len;
	const unsigned char *name;
};


//} :: include/linux/dcache.h:




:: fs/open.c:
//{

long do_sys_open(int dfd, const char __user *filename, int flags, int mode)
{
	...;
	char *tmp = getname(filename);
	int fd = PTR_ERR(tmp);

	if (!IS_ERR(tmp)) {
		fd = get_unused_fd_flags(flags);
		if (fd >= 0) {
			struct file *f = do_filp_open(dfd, tmp, &op, lookup);
			if (IS_ERR(f)) {
				put_unused_fd(fd);
				fd = PTR_ERR(f);
			}
			else {
				fsnotify_open(f);
				fd_install(fd, f);
			}
		}
		putname(tmp);
	}
	return fd;
}


putname()
PTR_ERR()
ERR_PTR()



//} :: fs/open.c:


:: fs/naimi.c:
//{

char * getname(const char __user * filename)
{
	return getname_flags(filename, 0);
}

static char *getname_flags(const char __user *filename, int flags)
{
	char *tmp, *result;

	result = ERR_PTR(-ENOMEM);
	tmp = __getname();
	if (tmp) {
		int retval = do_getname(filename, tmp);
		result = tmp;
		if (retval < 0) {
			if (retval != -ENOENT || !(flags & LOOKUP_EMPTY)) {
				__putname(tmp);
				result = ERR_PTR(retval);
			}
		}
	}
	audit_getname(result);
	return result;
}



static int do_getname(const char __user *filename, char *page)
{
	int retval;
	unsigned long len = PATH_MAX;

	if (!segment_eq(get_fs(), KERNEL_DS)) {
		if ((unsigned long) filename >= TASK_SIZE)
			return -EFAULT;
		if (TASK_SIZE - (unsigned long) filename < PATH_MAX)
			len = TASK_SIZE - (unsigned long) filename;
	}

	retval = strncpy_from_user(page, filename, len);
	if (retval > 0) {
		if (retval < len)
			return 0;
		return -ENAMETOOLONG;
	} else if (!retval)
		retval = -ENOENT;
	return retval;
}



struct file * do_filp_open(int dfd, const char *pathname, const struct open_flags *op, int flags)
{
	struct nameidata nd;
	struct file *filp;

	filp = path_openat(dfd, pathname, &nd, op, flags | LOOKUP_RCU);
	if (unlikely(filp == ERR_PTR(-ECHILD)))
		filp = path_openat(dfd, pathname, &nd, op, flags);
	if (unlikely(filp == ERR_PTR(-ESTALE)))
		filp = path_openat(dfd, pathname, &nd, op, flags | LOOKUP_REVAL);
	return file;
}

//} :: fs/naimi.c:




:: arch/x86/lib/usercopy_64.c:
//{

long
strncpy_from_user(char *dst, const char __user *src, long count)
{
	long res = -EFAULT;
	if (access_ok(VERIFY_READ, src, 1))
		return __strncpy_from_user(dst, src, count);
	return res;
}

long
__strncpy_from_user(char *dst, const char __user *src, long count)
{
	long res;
	__do_strncpy_from_user(dst, src, count, res);
	return res;
}


/*     
 * Copy a null terminated string from userspace.     
 */     
    
#define __do_strncpy_from_user(dst,src,count,res)                          \     
do {                                                                       \     
        long __d0, __d1, __d2;                                             \     
        might_fault();                                                     \     
        __asm__ __volatile__(                                              \     
                "       testq %1,%1\n"                                     \     
                "       jz 2f\n"                                           \     
                "0:     lodsb\n"                                           \     
                "       stosb\n"                                           \     
                "       testb %%al,%%al\n"                                 \     
                "       jz 1f\n"                                           \     
                "       decq %1\n"                                         \     
                "       jnz 0b\n"                                          \     
                "1:     subq %1,%0\n"                                      \     
                "2:\n"                                                     \     
                ".section .fixup,\"ax\"\n"                                 \     
                "3:     movq %5,%0\n"                                      \     
                "       jmp 2b\n"                                          \     
                ".previous\n"                                              \     
                _ASM_EXTABLE(0b,3b)                                        \     
                : "=&r"(res), "=&c"(count), "=&a" (__d0), "=&S" (__d1),    \     
                  "=&D" (__d2)                                             \     
                : "i"(-EFAULT), "0"(count), "1"(count), "3"(src), "4"(dst) \     
                : "memory");                                               \     
} while (0)    


//} :: arch/x86/lib/usercopy_64.c:




:: fs/ext4/inode.c:
//{

void ext4_set_aops(struct inode *inode)
{
	if (ext4_should_order_data(inode) &&
			test_opt(inode->i_sb, DELALLOCK))
		inode->i_mapping->a_ops = &ext4_da_aops;
}




static const struct address_space_operations ext4_ordered_aops = {
	.readpage		= ext4_readpage,
	.readpages		= ext4_readpages,
	.writepage		= ext4_writepage,
	.write_begin		= ext4_write_begin,
	.write_end		= ext4_ordered_write_end,
	.bmap			= ext4_bmap,
	.invalidatepage		= ext4_invalidatepage,
	.releasepage		= ext4_releasepage,
	.direct_IO		= ext4_direct_IO,
	.migratepage		= buffer_migrate_page,
	.is_partially_uptodate  = block_is_partially_uptodate,
	.error_remove_page	= generic_error_remove_page,
};

static const struct address_space_operations ext4_writeback_aops = {
	.readpage		= ext4_readpage,
	.readpages		= ext4_readpages,
	.writepage		= ext4_writepage,
	.write_begin		= ext4_write_begin,
	.write_end		= ext4_writeback_write_end,
	.bmap			= ext4_bmap,
	.invalidatepage		= ext4_invalidatepage,
	.releasepage		= ext4_releasepage,
	.direct_IO		= ext4_direct_IO,
	.migratepage		= buffer_migrate_page,
	.is_partially_uptodate  = block_is_partially_uptodate,
	.error_remove_page	= generic_error_remove_page,
};

static const struct address_space_operations ext4_journalled_aops = {
	.readpage		= ext4_readpage,
	.readpages		= ext4_readpages,
	.writepage		= ext4_writepage,
	.write_begin		= ext4_write_begin,
	.write_end		= ext4_journalled_write_end,
	.set_page_dirty		= ext4_journalled_set_page_dirty,
	.bmap			= ext4_bmap,
	.invalidatepage		= ext4_invalidatepage,
	.releasepage		= ext4_releasepage,
	.is_partially_uptodate  = block_is_partially_uptodate,
	.error_remove_page	= generic_error_remove_page,
};

static const struct address_space_operations ext4_da_aops = {
	.readpage		= ext4_readpage,
	.readpages		= ext4_readpages,
	.writepage		= ext4_writepage,
	.writepages		= ext4_da_writepages,
	.write_begin		= ext4_da_write_begin,
	.write_end		= ext4_da_write_end,
	.bmap			= ext4_bmap,
	.invalidatepage		= ext4_da_invalidatepage,
	.releasepage		= ext4_releasepage,
	.direct_IO		= ext4_direct_IO,
	.migratepage		= buffer_migrate_page,
	.is_partially_uptodate  = block_is_partially_uptodate,
	.error_remove_page	= generic_error_remove_page,
};




static sector_t ext4_bmap(struct address_space *mapping, sector_t block)
{
	struct inode *inode = mapping->host;
	journal_t *journal;
	int err;

	if (mapping_tagged(mapping, PAGECACHE_TAG_DIRTY) &&
			test_opt(inode->i_sb, DELALLOCK)) {
		filemap_write_and_wait(mapping);
	}

	if (EXT4_JOURNAL(inode) &&
			ext4_test_inode_state(inode, EXT4_STATE_JDATA)) {
		...;
		if (err)
			return 0;
	}

	return generic_block_bmap(mapping, block, ext4_get_block);
}




static int ext4_readpage(struct file *file, struct page *page)
{
	trace_ext4_readpage(page);
	return mpage_readpage(page, ext4_get_block);
}

//} :: fs/ext4/inode.c:




//{ :: fs/mpage.c:

int mpage_readpage(struct page *page, get_block_t get_block)
{
	struct bio *bio = NULL;
	sector_t last_block_in_bio = 0;
	struct buffer_head map_bh;
	unsigned long first_logical_block = 0;

	map_bh.b_state = 0;
	map_bh.b_size = 0;
	bio = do_mpage_readpage(bio, page, 1, &last_block_in_bio,
			&map_bh, &first_logical_block, get_block);

	if (bio)
		mpage_bio_submit(READ, bio);

	return 0;
}




static struct bio *
do_mpage_readpage(
		struct bio *bio,
		struct page *page,
		unsigned nr_pages,
		sector_t *last_block_in_bio,
		struct buffer_head *map_bh,
		unsigned long *first_logical_block,
		get_block_t get_block
		)
{
	struct inode *inode = page->mapping->host;
	const unsigned blkbits = inode->i_blkbits;
	const unsigned blocks_per_page = PAGE_CACHE_SIZE >> blkbits;
	const unsigned blocksize = 1 << blkbits;
	sector_t block_in_file;
	sector_t last_block;
	sector_t last_block_in_file;
	sector_t blocks[MAX_BUF_PER_PAGE];
	struct block_device *bdev = NULL;
	int length;
	int fully_mapped = 1;
	unsigned nblocks;
	unsigned relative_block;

	if (page_has_buffers(page))
		goto confused;

	block_in_file = (sector_t)page->index << (PAGE_CACHE_SHIFT - blkbits);




	...;

}




//} :: fs/mpage.c:




:: mm/vmscan.c:
//{



/*
 * This is a basic per-zone page freer.  Used by both kswapd and direct reclaim.
 */
static void shrink_zone(int priority, struct zone *zone,
				struct scan_control *sc)
{
	unsigned long nr[NR_LRU_LISTS];
	unsigned long nr_to_scan;
	enum lru_list l;
	unsigned long nr_reclaimed, nr_scanned;
	unsigned long nr_to_reclaim = sc->nr_to_reclaim;

restart:
	nr_reclaimed = 0;
	nr_scanned = sc->nr_scanned;
	get_scan_count(zone, sc, nr, priority);

	while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] ||
					nr[LRU_INACTIVE_FILE]) {
		for_each_evictable_lru(l) {
			if (nr[l]) {
				nr_to_scan = min_t(unsigned long,
						   nr[l], SWAP_CLUSTER_MAX);
				nr[l] -= nr_to_scan;

				nr_reclaimed += shrink_list(l, nr_to_scan,
							    zone, sc, priority);
			}
		}
		/*
		 * On large memory systems, scan >> priority can become
		 * really large. This is fine for the starting priority;
		 * we want to put equal scanning pressure on each zone.
		 * However, if the VM has a harder time of freeing pages,
		 * with multiple processes reclaiming pages, the total
		 * freeing target can get unreasonably large.
		 */
		if (nr_reclaimed >= nr_to_reclaim && priority < DEF_PRIORITY)
			break;
	}
	sc->nr_reclaimed += nr_reclaimed;

	/*
	 * Even if we did not try to evict anon pages at all, we want to
	 * rebalance the anon lru active/inactive ratio.
	 */
	if (inactive_anon_is_low(zone, sc))
		shrink_active_list(SWAP_CLUSTER_MAX, zone, sc, priority, 0);

	/* reclaim/compaction might need reclaim to continue */
	if (should_continue_reclaim(zone, nr_reclaimed,
					sc->nr_scanned - nr_scanned, sc))
		goto restart;

	throttle_vm_writeout(sc->gfp_mask);
}




//} :: mm/vmscan.c:




//{ :: include/linux/blkdev.h

struct request_queue {

	/* together with queue_head for cacheline sharing */
	struct list_head		queue_head;
	struct request			*last_merge;
	struct elevator_queue		*elevator;

	/* the queue request freelist, one for reads and one for writes */
	struct request_list		rq;

	request_fn_proc			*request_fn;
	make_request_fn			*make_request_fn;
	prep_rq_fn			*prep_rq_fn;
	unprep_rq_fn			*unprep_rq_fn;
	merge_bvec_fn			*merge_bvec_fn;
	softirq_done_fn			*softirq_done_fn;
	rq_timed_out_fn			*rq_timed_out_fn;
	dma_drain_needed_fn		*dma_drain_needed;
	lld_busy_fn			*lld_busy_fn;

	/* dispatch queue sorting */
	sector_t			end_sector;
	struct request			*boundary_rq;

	/* delayed queue handling */
	struct delayed_work		delay_work;

	struct backing_dev_info		backing_dev_info;

	/* queue owner gets to use this for whatever they like.
	 * ll_rw_blk doesn't touch it
	 */
	void				*queuedata;

	/* queue needs bounce pages for pages above this limit */
	gfp_t				bounce_gfp;

	/* various queue flags, see QUEUE_* below */
	unsigned long			queue_flags;

	/*
	 * protects queue structures from reentrancey.
	 * __queue_lock should never be used directly. it is queue private.
	 * please use queue_lock always.
	 */
	spinlock_t			__queue_lock;
	spinlock_t			*queue_lock;

	/* queue kobject */
	struct kobject kobj;

	/* queue settings */
	unsigned long			nr_requests;	/* max # of requests */
	unsigned int			nr_congestion_on;
	unsigned int			nr_congestion_off;
	unsigned int			nr_batching;

	void				*dma_drain_buffer;
	unsigned int			dma_drain_size;
	unsigned int			dma_pad_mask;
	unsigned int			dma_alignment;

	struct blk_queue_tag		*queue_tags;
	struct list_head		tag_busy_list;

	unsigned int			nr_sorted;
	unsigned int			in_flight[2];

	unsigned int			rq_timeout;
	struct timer_list		timeout;
	struct list_head		timeout_list;

	struct queue_limits		limits;

	/* sg stuff */
	unsigned int			sg_timeout;
	unsigned int			sg_reserved_size;
	int				node;
#ifdef CONFIG_BLK_DEV_IO_TRACE
	struct blk_trace		*blk_trace;
#endif

	/* for flush operations */
	unsigned int			flush_flags;
	unsigned int			flush_not_queueable:1;
	unsigned int			flush_queue_delayed:1;
	unsigned int			flush_pending_idx:1;
	unsigned int			flush_running_idx:1;
	unsigned long			flush_pending_since;
	struct list_head		flush_queue[2];
	struct list_head		flush_data_in_flight;
	struct request			flush_rq;

	struct mutex			sysfs_lock;

}; // struct request_queue {};

//} :: include/linux/blkdev.h


typedef unsigned __bitwise__ gfp_t;
typedef unsigned __bitwise__ fmode_t;


:: block/blk-lib.c:
//{


int blkdev_issue_discard(struct block_device *bdev, sector_t sector,
		sector_t nr_sects, gfp_t gfp_mask, unsigned long flags)
{

	...;
#if 1 /* { BLUSJUNE_CODE_ZONE_OPEN */
        printk("BLUSJUNE_CODE::\tblkdev_issue_discard(): \t sector== %lu \t nr_sects== %lu\n",
                        sector, nr_sects);
#endif /* } BLUSJUNE_CODE_ZONE_CLOSE */
	...;
}


//} :: block/blk-lib.c:







:: 
{ set_ftrace_filter
____pagevec_lru_add			## too_frequently_being_called
____pagevec_lru_add_fn			## too_frequently_being_called
__fscache_acquire_cookie
__fscache_alloc_page
__fscache_attr_changed
__fscache_check_page_write
__fscache_cookie_put
__fscache_lookup_cache_tag
__fscache_maybe_release_page
__fscache_read_or_alloc_page
__fscache_read_or_alloc_pages
__fscache_register_netfs
__fscache_release_cache_tag
__fscache_relinquish_cookie
__fscache_uncache_all_inode_pages
__fscache_uncache_page
__fscache_unregister_netfs
__fscache_update_cookie
__fscache_wait_on_page_write
__fscache_write_page
__pagevec_free
__pagevec_release
activate_page
add_page_to_unevictable_list
add_to_page_cache_lru
check_move_unevictable_page
deactivate_page
delete_from_lru_cache
do_munmap
do_read_cache_page
do_sync_read	##
do_sync_write	##
find_or_create_page
free_pages_and_swap_cache
generic_file_aio_read
generic_file_aio_write
mark_page_accessed			## too_frequently_being_called
nr_free_pagecache_pages
pagevec_lookup				## too_frequently_being_called
pagevec_lookup_tag			## too_frequently_being_called
pagevec_lru_move_fn
pagevec_move_tail			## too_frequently_being_called
pagevec_move_tail_fn			## too_frequently_being_called
pagevec_strip				## too_frequently_being_called
read_cache_pages
read_cache_pages_invalidate_page
release_pages
shrink_zone
unmap_region
unmap_vmas				## too_frequently_being_called
vfs_read	##
vfs_write	##
write_cache_pages
write_cache_pages_da

}
